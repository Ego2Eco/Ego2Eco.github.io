<!DOCTYPE html>
<html>
  <head>
    <title>AEE-GAN</title>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="style.css" />
  </head>
<div class="demo">
  <h1><center>Attended Ecology Embedding Generative Adversarial Network</h1>
  <h1><font size="5"><center>Implementation details</font></h1>
  <p>For Feature Encoder, we extract the visual features by fully convolutional networks and derive the feature maps with 512 channels, while the observed trajectories are encoded by a fully connected layer with the embedding size of 5*64, fed into an LSTM with a dimension of 64 for the hidden state. The social features are embedded through a fully-connected layer with the embedding size 64. </p>
    <p>For Enforced Attention, the self-attention visual features are encoded by a 3*3 convolutional layer and one fully-connected layer with the dimension of 64. c' is set as c/8 due to the memory efficiency. Horizon Attention considers the road-agents in the horizon range within 10 meters, i.e., d=10. The decoder of generator uses LSTM with the hidden state of 64 dimensions to decode the predictions, while the discriminator uses two LSTM encoders with the hidden size of 64 dimensions to separately encode predicted trajectories and the observed path, and are further processed through two fully-connected layers with a size of 64 and 32.</p> 
  <p>Finally, these outputs are concatenated and fed in two separate blocks: Trajectory Classifier and Latent Code Decoder which are consists of two fully-connected layers with respective size of (32,1) and (32,2). The proposed AEE-GAN is trained for 20000 epochs by Adam optimizer with a mini-batch size of 256. The learning rates for the generator and the discriminator are both 0.0001, and spectral normalization is used for layers in both generator and the discriminator.</p>
  <h1><font size="5"><center>Modified Figure 1</font></h1>
    <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/attention_revise.png?raw=true" width="720" height="236"/></figure>
    <p>Illustration of the enforced attention mechanism in AEE-GAN in the heterogeneous environment. In the right figure, we show that the pedestrians in the horizon region (yellow region) are more attended than other road-agents by our model, and AEE-GAN is able to attend the related visual features for prediction, where the white portions indicate the attended region. The left image presents the predicted trajectories in the traffic scene.</p>
  <h1><font size="5"><center>Prediction Results of AEE-GAN</font></h1>
  <p> <center>Green: AEE-GAN, PINK: TraPHic, Blue: GT</p></center>
  <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/3.gif?raw=true" width="640" height="480"/></figure>
  <p>The motion pattern of the right-turned white car, which is slowed down to wait for the passing pedestrians and completed the right turn.</p>
  <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/15.gif?raw=true" width="640" height="480"/></figure>
  <p>The motion pattern of the right-turned red car in the middle of the intersection has been successfully predicted by our model, where the trajectories of the follow-up cars have also been accurately predicted, indicating that the interactions in the complex intersection scene could be captured by our model.</p>
  <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/25.gif?raw=true" width="640" height="480"/></figure>
  <p>The prediction results of the white pedestrians from the right were initially moving upward. Once the interaction of the upcoming pedestrians in blue and orange was sensed, our model predicts the descending trajectories to avoid the collision.</p>
  
<h1><font size="5"><center>Visualizations of Visual Attention</font></h1></center>
  
  <p>We use Grad-Cam to visualize our result of visual attention. The heatmap represents the attention weights toward the image pixels, which the warmer it gets indicates that the higher attention weight it is. The predicted result of trajectory and ground truth is colored in black green, respectively.</p>
  <p><center>Green: Ground Truth, Black: Prediction</p></center>
  <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/12.png?raw=true" width="640" height="480"/></figure>
  <p>The result of the heatmap shows that the attended region in the left, where exists a physical constraint leading the traffic agent to move aside.</p>
  <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/15cam.png?raw=true" width="640" height="480"/></figure>
  <p>The result shows that through attending the upper region, it follows the moving pattern in a roundabout while attending the lower region on the sidewalk, the road-agent is predicted to stay in the roundabout.</p>
<h1><font size="5"><center>Qualitative Results</font></h1>
    <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/visual.png"  width="640" height="480"/></figure>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/horizon.png"  width="640" height="480"/></figure>
      <p><center>Blue: Ground truth; Dotted line of other colors: Observed Trajectory; Solid line of other colors: Prediction Trajectory</p></center>
      In the first scenario, figure (a) visualizes the attention weights of the Social Enforcement module, and figure (b) shows the prediction trajectories. S-WAYS fails to emphasize on the road-agents in front of the predicted agent but only focuses on the neighboring agent. As such, the red predicted trajectory occur collision. In contrast, our model can provide a more appropriate attention weights distribution that allows the predicted agent to notice important regions and predict a movement to avoid the collision.
<p>In the second scenario, figure (c) visualizes the attention weights of the Social Enforcement module, and figure (d) shows the prediction trajectories. S-WAYS fails to emphasize on the neighboring road-agents with the same walking direction next to the predicted agent; instead, it focuses on the neighboring agent behind. In contrast, our model can provide well-balanced weights on the neighboring agents which help predict more socially plausible trajectories compared to the other.</p>  

  <h1><font size="5"><center>More Results of Waymo Dataset</h1></center>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_3654.png" width="640" height="480"/></figure>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_3624.png" width="640" height="480"/></figure>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_2820.png" width="640" height="480"/></figure>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_2475.png" width="640" height="480"/></figure>
     <h1><font size="5"><center>More Results of SDD</h1></center>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/deathCircle0.png" width="640" height="480"/></figure>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/gate3.png" width="640" height="480" /></figure>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/hyang1.png" width="640" height="480"/></figure>
      <h1><center><font size="5">Prediction Results of Homogeneous dataset</h1>
      <p><center>Blue: Ground truth; Other colors: Prediction Result</p></center>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_8761.png" width="640" height="480"/></figure>
       <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_4793.png" width="640" height="480"/></figure>
       <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_6875.png" width="640" height="480"/></figure>
       <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_7829.png" width="640" height="480"/></figure>
  
</div>
