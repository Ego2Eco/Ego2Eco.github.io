<!DOCTYPE html>
<html>
  <head>
    <title>AEE-GAN</title>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="style.css" />
  </head>
<div class="demo">
  <h1><center>Attended Ecology Embedding Generative Adversarial Network</h1>
  <h1><font size="5"><center>Implementation details</font></h1>
  <p>For Feature Encoder, we extract the visual features by fully convolutional networks and derive the feature maps with 512 channels, while the observed trajectories are encoded by a fully connected layer with the embedding size of 5*64, fed into an LSTM with a dimension of 64 for the hidden state. The social features are embedded through a fully-connected layer with the embedding size 64. </p>
    <p>For Enforced Attention, the self-attention visual features are encoded by a 3*3 convolutional layer and one fully-connected layer with the dimension of 64. c' is set as c/8 due to the memory efficiency. Horizon Attention considers the road-agents in the horizon range within 10 meters, i.e., d=10. The decoder of generator uses LSTM with the hidden state of 64 dimensions to decode the predictions, while the discriminator uses two LSTM encoders with the hidden size of 64 dimensions to separately encode predicted trajectories and the observed path, and are further processed through two fully-connected layers with a size of 64 and 32.</p> 
  <p>Finally, these outputs are concatenated and fed in two separate blocks: Trajectory Classifier and Latent Code Decoder which are consists of two fully-connected layers with respective size of (32,1) and (32,2). The proposed AEE-GAN is trained for 20000 epochs by Adam optimizer with a mini-batch size of 256. The learning rates for the generator and the discriminator are both 0.0001, and spectral normalization is used for layers in both generator and the discriminator.</p>
  <h1><font size="5"><center>Modified Figure 1</font></h1>
    <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/attention_revise.png?raw=true" width="640" height="236"/></figure>
    <p>Illustration of the enforced attention mechanism in AEE-GAN in the heterogeneous environment. In the right figure, we show that the pedestrians in the horizon region (yellow region) are more attended than other road-agents by our model, and AEE-GAN is able to attend the related visual features for prediction, where the white portions indicate the attended region. The left image presents the predicted trajectories in the traffic scene.</p>
  <h1><font size="5"><center>Prediction Results of AEE-GAN</font></h1>
  <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/3.gif?raw=true" width="640" height="480"/></figure>
  <p>In this example, ground truth is colored in blue, and the predicted trajectory of our model is colored in green compared with the result of TraPHic in pink. The result demonstrates that our model could capture the complex interactions by predicting decent trajectories of the crowd, and our model simultaneously captures the motion pattern of the right-turned white car, which is slowed down to wait for the passing pedestrians and completes the right turn.</p>
  <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/15.gif?raw=true" width="640" height="480"/></figure>
  <p>In this example, ground truth is colored in blue, and the predicted trajectory of our model is colored in green compared with the result of TraPHic in pink. The result shows the motion pattern of the right-turned red car in the middle of the intersection has been successfully predicted by our model, where the trajectories of the follow-up cars have also been accurately predicted, indicating that the interaction of road-agents in the complex intersection scene could be captured by our model.</p>
  <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/25.gif?raw=true" width="640" height="480"/></figure>
  <p>In this example, the past trajectory is colored in blue, and the predicted trajectory is colored in green with the ground truth in red. The experiment result shows that the pedestrians in white from the right side were initially moving upward. Once the interaction of the upcoming pedestrians in blue and orange was sensed, our model generates descending trajectories to avoid the collision.</p>
  
<h1><font size="5"><center>Visualizations of Visual Attention</font></h1></center>
  
  <p>We use Grad-Cam to visualize our result of visual attention. The heatmap represents the attention weights toward the image pixels, which the warmer it gets indicates that the higher attention weight it is. The predicted result of trajectory and ground truth is colored in black green, respectively.</p>
  <p><center>Green: Ground Truth, Black: Prediction</p></center>
  <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/12.png?raw=true" width="640" height="480"/></figure>
  <p>The result of the heatmap shows that the attended region in the left, where exists a physical constraint leading the traffic agent to move aside.</p>
  <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/15cam.png?raw=true" width="640" height="480"/></figure>
  <p>The result shows that through attending the upper region, it follows the moving pattern in a roundabout while attending the lower region on the sidewalk, the road-agent is predicted to stay in the roundabout.</p>
    
    <h1><font size="5"><center>Ablation Study</font></h1>
    <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/ablation%20study.png?raw=true"  width="640" height="50"/></figure>
    <p>We conduct the ablation study by leave-one-out analysis on the ETH dataset, i.e., our complete model (AEE-GAN), AEE-GAN without using Social Enforcement module SEo, AEE-GAN without using Recurrent Visual Attention Enforcement module RVAEo, AEE-GAN without recurrently computing the attention between the predicted results and the visual features Ro, AEE-GAN without using Horizon Attention module Ho, AEE-GAN without using the Self Attention module So.</p>
    
      <h1><font size="5"><center>Time Table</font></h1></center>
  <figure><img src="https://github.com/Ego2Eco/Ego2Eco.github.io/blob/master/images/time%20table.png?raw=true"  width="640" height="85"/></figure>
  <p>Computation time comparison table. In this table, we compare the execution time of our model with three trajectory prediction methods, i.e., S-GAN-P, S-Ways, TraPHic, and AEE-GAN. The execution time is reported in seconds. As mentioned in Q6, our model takes more computation time than other methods since we have to consider the interaction between agents and scenes. It is one of the limitations that we want to improve in the future.</p>
      
<h1><font size="5"><center>Qualitative Results</font></h1>
    <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/visual.png"  width="640" height="480"/></figure>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/horizon.png"  width="640" height="480"/></figure>
      <p><center>Blue: Ground truth; Dotted line of other colors: Observed Trajectory; Solid line of other colors: Prediction Trajectory</p></center>
      In the first scenario, figure (a) visualizes the attention weights of the Social Enforcement module, and figure (b) shows the prediction trajectories. S-WAYS fails to emphasize on the road-agents in front of the predicted agent but only focuses on the neighboring agent. As such, the red predicted trajectory occur collision. In contrast, our model can provide a more appropriate attention weights distribution that allows the predicted agent to notice important regions and predict a movement to avoid the collision.
<p>In the second scenario, figure (c) visualizes the attention weights of the Social Enforcement module, and figure (d) shows the prediction trajectories. S-WAYS fails to emphasize on the neighboring road-agents with the same walking direction next to the predicted agent; instead, it focuses on the neighboring agent behind. In contrast, our model can provide well-balanced weights on the neighboring agents which help predict more socially plausible trajectories compared to the other.</p>  

  <h1><font size="5"><center>More Results of Waymo Dataset</h1></center>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_3654.png" width="640" height="480"/></figure>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_3624.png" width="640" height="480"/></figure>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_2820.png" width="640" height="480"/></figure>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_2475.png" width="640" height="480"/></figure>
     <h1><font size="5"><center>More Results of SDD</h1></center>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/deathCircle0.png" width="640" height="480"/></figure>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/gate3.png" width="640" height="480" /></figure>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/hyang1.png" width="640" height="480"/></figure>
      <h1><center><font size="5">Prediction Results of Homogeneous dataset</h1>
      <p><center>Blue: Ground truth; Other colors: Prediction Result</p></center>
      <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_8761.png" width="640" height="480"/></figure>
       <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_4793.png" width="640" height="480"/></figure>
       <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_6875.png" width="640" height="480"/></figure>
       <figure><img src="https://raw.githubusercontent.com/Ego2Eco/Ego2Eco.github.io/master/images/predict_7829.png" width="640" height="480"/></figure>
  
</div>
